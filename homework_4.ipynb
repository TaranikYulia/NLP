{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlu-s2k9D1Ba"
      },
      "source": [
        "# Week 4: Transfer Learning, BERT (Homework)\n",
        "\n",
        "## Question Search Engine\n",
        "\n",
        "Embeddings are a good source of information for solving various tasks. For example, we can classify texts or find similar documents using their representations. We already know about word2vec, GloVe and fasttext, but they don't use context information from given text (only from contexts of source data).\n",
        "\n",
        "For today we will use full power of context-aware embeddings to find text duplicates!\n",
        "\n",
        "__Warning:__ this task assumes you have seen `seminar.ipynb`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HYffoHiI8du5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfSHyQlT-fVF"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469,
          "referenced_widgets": [
            "1119b4ccebb2491ea0ade658e2809b5c",
            "d0dade9fcac74431b2c1da554017a375",
            "7176d10bfdea45e8bd410c1799e2b67d",
            "b83d30761ba74b0fb5cb5196ef26f8a3",
            "87a522aad38d46be824e24be676e9f30",
            "7b873f7bb8e24e91ab2aa76c77919251",
            "f8882e6609204cd689204705cae412bf",
            "9aeebca34cbc47389317fc35408e0b2f",
            "aef629d461aa4ed0a5635f0f63b018b6",
            "71bc0b2d3d2845afae4de1b8139630ce",
            "810474bc80104bb9952c5a4cfeb68670",
            "8238a0c79e77483bbcdfb6c7f97c9a1d",
            "7d12330bb22241cb9b0518b654e63c4d",
            "9ca32725f13c4b1aba7592d89a288421",
            "9436732a3cdb441183c762b31696ccac",
            "b0fc12b337fa4aa597c50e5f586b1fbc",
            "184bfa596ef34308a7246775688193f9",
            "23e79f9d018f488d941ff8ee18741094",
            "dddbd41fe1e54dba8c588451c30dcb84",
            "67bdd2f388fa45259a0234acd87316a2",
            "2fd5c871a42d4c72ae07e5c636444d79",
            "253e33705fa84f299533c03007bd65cd",
            "091f92f2bd8443b2b7314806c8917c2b",
            "814673eddd404d27b790b87565d84a42",
            "36313d9747b247e193b5a34d2c3384c2",
            "734a0d22127346ab8ef328c8bfbf0423",
            "38fd561ae6d54d55820fc210412eaec2",
            "e53203597bd742e8ba71c7a2bf77c834",
            "bf498673cc114860adf406d2c9964882",
            "60f7751807584285abf9924ad08c16fd",
            "0449ec3108db4cd68acc76f1d9bf4578",
            "bbf07029349d453197676c6473150925",
            "3e8a5d9fbf8348938d5051976f17aada",
            "72315a8552ce4f48849263ec32dc2e3b",
            "c8c973ecb4214b9fae56f0dfab3afb73",
            "ca8888cba63a4130b8ee69b63ff7d3bc",
            "95cfd596de8e4d87b4092269dcdbc7f3",
            "c78f0a07628e4613aa12931f40c9fad0",
            "2d7e51b415c543b9941591599b97e506",
            "9934f85508954727b3368d257d231028",
            "9b1106042f5d42be87cd8d0ccbe0af37",
            "3bd763ba953346c6b06be70e3df94aa1",
            "f58aea4910b345f0b858870ff8f1b2c3",
            "a01a8e4946a9461b8730b4721635a9ad",
            "73378c2707a54265957c2f9baa1f8d14",
            "d736efc9553a4331b027bc491a2039bf",
            "23dac02918e84c3e99a66fd7ac02602a",
            "109a21ee04bd4028a0bd26688a43dc0a",
            "8a0cbaa9e3f343aeb2b6e32969c92887",
            "42797b2261684a9ab4472f0305484fe1",
            "daf287d004c44ffca55a52b14ceef838",
            "fa789408a18c4036ada1c7d7019267f8",
            "404893d41c7a4b1980d6e04731f2bebf",
            "5334384d88ae42e49d1a51ec4f5ac144",
            "a2364ff4ee45423cac1e166a5999c7e6",
            "0d8d5b91020148c7906f947593520380",
            "6c21789647174d7a9628b41106abad28",
            "bf3477e8cc76462ca380a6a12b7c06c0",
            "164ede6c9eca4801b965787ed46ec648",
            "d16b5e053f6d4094b55ff792f0fe17e1",
            "eea5a63a6bb148e0bffc6e3b3868c934",
            "e6e167279ceb468ca7f91727ab092d41",
            "4bd4f712762e462cb73d720f26d4307c",
            "5c68852b13cf4094896331183622b2d7",
            "6acafd9d2ca248feb6fcac611ed9add3",
            "6c3d7ecfd4fe4254af5c8a94c7cb56c8",
            "5ba2345c606444f6b5c34affbcfcee01",
            "545d144db853484c8a4c0d6c823d7b51",
            "bb6e70565c1b4418b77ab96949220706",
            "34d4913a252a48ff90e55f35078cc909",
            "55f258a268414b92a44abf8f437957b3",
            "5392c025bd43408e8ec3fa1d32edc7b3",
            "be58eec5dcb249009b798684c97e5328",
            "faf86fa664814d43b22404d2e0d0f54b",
            "a84aebaa91784d91aa9393958d107d6f",
            "b32c2533add94e889aab669cf2b16228",
            "e50724c414a3415b9c625f24fd9a7773"
          ]
        },
        "id": "Y2_wgtrx8e6C",
        "outputId": "6637deb9-6521-4e07-be84-df09101889d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1119b4ccebb2491ea0ade658e2809b5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/313 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8238a0c79e77483bbcdfb6c7f97c9a1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train.jsonl:   0%|          | 0.00/70.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "091f92f2bd8443b2b7314806c8917c2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation.jsonl: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72315a8552ce4f48849263ec32dc2e3b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test.jsonl:   0%|          | 0.00/76.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73378c2707a54265957c2f9baa1f8d14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/363846 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d8d5b91020148c7906f947593520380",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/40430 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ba2345c606444f6b5c34affbcfcee01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/390965 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Sample[0]: {'text1': 'How is the life of a math student? Could you describe your own experiences?', 'text2': 'Which level of prepration is enough for the exam jlpt5?', 'label': 0, 'idx': 0, 'label_text': 'not duplicate'}\n",
            "Sample[3]: {'text1': 'What can one do after MBBS?', 'text2': 'What do i do after my MBBS ?', 'label': 1, 'idx': 3, 'label_text': 'duplicate'}\n"
          ]
        }
      ],
      "source": [
        "qqp = datasets.load_dataset(\"SetFit/qqp\")\n",
        "print(\"\\n\")\n",
        "print(\"Sample[0]:\", qqp[\"train\"][0])\n",
        "print(\"Sample[3]:\", qqp[\"train\"][3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "4cf601702df84cb1bd5bbc29e8b9f25d",
            "d0b1e0b6e77041c99295aeb256c126ec",
            "fe4708266acf4a6c990b5123d1b04029",
            "19996d29b8d44ffab61c561f159bc242",
            "8d8d1b7a21b94d1f9f90f1697e60dac2",
            "f69edc1a5b8d4f9e84939dcad00af025",
            "1be8bed8277b4f88a4165dcbdc2dd88a",
            "6ffac6a0cee74094b296639cce72bf9b",
            "282519e32b874197913f7c3bf0bf6f17",
            "8bfb77f02971401ca26b0298c3f2f1c4",
            "a3f85bd18b4f4956977abc8008fee512",
            "5a649abf546a46bda9148e7116921b59",
            "062c90698e1241848ad284434d61a8ae",
            "3352a1c46982408b9041c2214a8c66f2",
            "6b44a29a5eb34ea78dc0811e1228c907",
            "339dfa0d6d3a4cd093e0cbaef72258fc",
            "440d21f55e6a44088977cf582268622f",
            "f48bc4b7fc424275bb0ce5bf2ae652ee",
            "a08c8839a15e4973821190e6992e6853",
            "2a52d0bc4ead4f8ebc75ca8c5a5e7c5e",
            "62d3a6fbd9b244c1948cdb755cd80af1",
            "53d5c280269a45d7abf2a916c50f437e",
            "945db8e9bfa344d6a2a9e3848631e64f",
            "16c9578a663c47b8b110e9fd7da8a442",
            "482bc5a9e3094accb11751607cd33737",
            "3e102c90d0f64ba1af80ebce349c5926",
            "0e3a1317c136492883133db98b1168e7",
            "76ac429bb1344ef19854a3af809b8c16",
            "f7ed182e6e5b4aee9957cd309d2a65ab",
            "d7d66f44fd89446fa79b04961ea9ff66",
            "5a985e13b9c042e0ae434cc34545417d",
            "2c4281ec39e245b7afc64c2f3358e697",
            "f1881bfe5afe40dc863e5f8c4fa82244",
            "ca7c4424f13946479e7f16f4539222e9",
            "465ba119c6d04f40aa8aa09dc4523a74",
            "7ba86607e85446c39734a3de5d7e2f5b",
            "ab81744a91564fc08889d9099a5a4ed7",
            "9033f9f919ef4346958e044af733067e",
            "bdd33a2d30a44de1b20b5b21a0086cdd",
            "023e3989944c4829a645ef2594141763",
            "9b75197e05bd417e923e25f159d98d9c",
            "fa32371d4c094a17952e418ce2c753dc",
            "8a5a234ce192495e99bd8c0d3661fcad",
            "6aa7ff22aac4471290f2bbb858b7f292",
            "2bf7d4e0559940638318df8988aeb21a",
            "bced5488b3314b7a81be7a91da6969de",
            "ee89c74817fb489fbc818c1bae93430a",
            "2c0198f5e8574301ae873c7e6e476ec0",
            "c25b9ead483043a5ad89b1d2cd2812f8",
            "257177e4996a4034881b9dd35078edd3",
            "630afb656f6e412e82471e54266d8da0",
            "70e289661d6d42a5a2f15ee022e71ed1",
            "5cc5f6a32ba34ce59590242045b80cf9",
            "b9f34361189b4f6b842d90f9c0d93990",
            "c7d18ed9bd1e40e3b4ff6140724b8a5e",
            "0e8f5438f6774351b5118dc037094040",
            "b4b3f1b72cb049d18272e8eae024b2ad",
            "ae6367ed9840452583f68ea633b8277d",
            "81c1ee52baf643cfaacc8a2b1e0b1301",
            "26d70b2a69ed43a990cfab61fec6c6b0",
            "9dc94dc35bbd48749a1e86af61b12174",
            "e5d0c7840142473c9ef747e91f687d9a",
            "bb6f66f4a1df49349eaeb922d4212395",
            "ce8cdc6c6e2d470abf916d8cd8f471cf",
            "94ecf275ceb34e0085bfd0c72b6d0df9",
            "1f91a063ae92416890e29edb372101be"
          ]
        },
        "id": "pStlWcvD8rdk",
        "outputId": "b49f99de-7473-43ba-e22a-59eabcdc81ce"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cf601702df84cb1bd5bbc29e8b9f25d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a649abf546a46bda9148e7116921b59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "945db8e9bfa344d6a2a9e3848631e64f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca7c4424f13946479e7f16f4539222e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bf7d4e0559940638318df8988aeb21a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/890 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e8f5438f6774351b5118dc037094040",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = \"gchhablani/bert-base-cased-finetuned-qqp\"\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qtkllSPG9bTL"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    result = tokenizer(\n",
        "        examples[\"text1\"],\n",
        "        examples[\"text2\"],\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "    result[\"label\"] = examples[\"label\"]\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "8e5b0991dd874070b436b35b89f177f3",
            "832e1f112507483ea4d272ce1f4e2dbf",
            "95a97783d8b4447c9fc6c5f77dddd214",
            "063e2b3522934d40b56b6663f492e11d",
            "78d6e16fffb5403982067261104afbdf",
            "c26e68fae6e04a358353a419ef34fa46",
            "d96f600ca0274dc08af4d719b2061b02",
            "63738258debc4162b0cf8dfc6144676d",
            "c4301736204b44a89eb47e51c400d39d",
            "4bcb48109d8845af9b331f50f6cc5eb1",
            "5687ce564c8347c1920d46a494619abe",
            "a0b5a049693543a983f03d5b6a9f6e98",
            "81b1261550344e0bbfa16f9159b133a1",
            "8d63e7925b4248eca7cd0aaaba026f77",
            "0beb4bc535f0425c9c851eb9e711224c",
            "cbc5f8831d4844f19ae43bde0d03afb3",
            "a4be26a69f7942c6bd54feaa24832d63",
            "df473fc73eb747e4ae88ffd95e97fa70",
            "36fa8d8143a1447da0d4e721b3679401",
            "1eee4015bbb34b00b7a7849cdf732cef",
            "88b66eca022c426c80a592969a4a7e09",
            "e16ee27c5fc542a9886c71013f456205",
            "98704081eda3449787c97906d9cd4117",
            "9297613a3ddb41dca54991fa0d98ed44",
            "ebac1dd6397b4817b00f53a0a3f5308d",
            "16dd8927645940e9907e5f3f86c8a644",
            "d6477b37f1804f06ad035a9876121d7b",
            "ad166d29afea44038f3a4ee42a98a191",
            "f1612e54850d4bf2b2d169f4ffeedaed",
            "8c462c6372ae44f4b9d8fa5ab56c0393",
            "227cb8c24ad34f94b917b4992328169a",
            "6eeb9ca15e1f4004a34e434ac4940c2e",
            "b6132d4a1ac84232aed5455abda94a94",
            "193b280b12f34f48b260797a899e9533",
            "84f234e82c644f668b897cc1e7baa725",
            "d0aadb4352b048129149241d6ba17239",
            "4b122f9fb90a43e8bb51ba6d6a0cfe46",
            "b993fc1cee8a459c837b1f83e8a5dcb9",
            "847610ebe2a64fc9b1691ee11f207471",
            "3c7b8889394445fbb686d050c4647cc3",
            "6bd9ffa5e8f8410e9099060459d5aed0",
            "28d51a73b3a946268927de25b2baa036",
            "166658fc04a448628d8abdc2c3865e49",
            "5d5e5af5b4484048af001fc98cb2c952"
          ]
        },
        "id": "3HdPHQe4RmWs",
        "outputId": "ed6aaf95-5485-4a28-bb3c-8df8091902dd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e5b0991dd874070b436b35b89f177f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/363846 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0b5a049693543a983f03d5b6a9f6e98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98704081eda3449787c97906d9cd4117",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40430 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "193b280b12f34f48b260797a899e9533",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/390965 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "qqp_preprocessed = qqp.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObMcFN59_Ll2",
        "outputId": "8ea61900-cd4c-424f-c24e-bbcc75d91f26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[101, 1731, 1110, 1103, 1297, 1104, 170, 12523, 2377, 136, 7426, 1128, 5594, 1240, 1319, 5758, 136,  ...\n"
          ]
        }
      ],
      "source": [
        "print(repr(qqp_preprocessed[\"train\"][0][\"input_ids\"])[:100], \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyQ1ZbzGAUF2"
      },
      "source": [
        "### Evaluation (1 point)\n",
        "\n",
        "We randomly chose a model trained on QQP - but is it any good?\n",
        "\n",
        "One way to measure this is with validation accuracy - which is what you will implement next.\n",
        "\n",
        "Here's the interface to help you do that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M5ueSoieAbBg"
      },
      "outputs": [],
      "source": [
        "val_set = qqp_preprocessed[\"validation\"]\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set, batch_size=1, shuffle=False, collate_fn=transformers.default_data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsPwXXx-At-i",
        "outputId": "5bd8e0fa-fa9e-4277-c0db-562721beadaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 40430/40430 [08:13<00:00, 81.85batch/s, Current Acc=0.9085]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Validation Accuracy: 0.9084\n",
            "Correct predictions: 36726/40430\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Check if CUDA is available and set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move model to GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# Evaluation code\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Используем tqdm для показа прогресса\n",
        "progress_bar = tqdm(val_loader, desc=\"Evaluating\", unit=\"batch\")\n",
        "\n",
        "for i, batch in enumerate(progress_bar):\n",
        "    # Move batch to GPU\n",
        "    batch = {key: value.to(device) for key, value in batch.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get model predictions\n",
        "        outputs = model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            token_type_ids=batch[\"token_type_ids\"],\n",
        "        )\n",
        "\n",
        "        # Get predicted class (0 or 1)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "        # Compare with true labels\n",
        "        true_labels = batch[\"labels\"]  # или batch[\"label\"]\n",
        "        correct_predictions += (predictions == true_labels).sum().item()\n",
        "        total_predictions += true_labels.size(0)\n",
        "\n",
        "        # Обновляем прогресс-бар с текущей точностью каждые 100 батчей\n",
        "        if (i + 1) % 100 == 0:\n",
        "            current_accuracy = correct_predictions / total_predictions\n",
        "            progress_bar.set_postfix({\"Current Acc\": f\"{current_accuracy:.4f}\"})\n",
        "\n",
        "# Calculate final accuracy\n",
        "accuracy = correct_predictions / total_predictions\n",
        "print(f\"\\nFinal Validation Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Correct predictions: {correct_predictions}/{total_predictions}\")\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    predicted = model(\n",
        "        input_ids=batch[\"input_ids\"],\n",
        "        attention_mask=batch[\"attention_mask\"],\n",
        "        token_type_ids=batch[\"token_type_ids\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoxHzxn0DQqO"
      },
      "source": [
        "**Task 1 (1 point)**\n",
        "\n",
        "- Measure the validation accuracy of your model. Doing so naively may take several hours. Please make sure you use the following optimizations:\n",
        "  - Run the model on GPU with no_grad\n",
        "  - Using batch size larger than 1\n",
        "  - Use optimize data loader with num_workers > 1\n",
        "  - (Optional) Use [mixed precision](https://pytorch.org/docs/stable/notes/amp_examples.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0R2z_-FZU3qy"
      },
      "outputs": [],
      "source": [
        "assert 0.9 < accuracy < 0.91"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KONQ1E0J-y6B"
      },
      "source": [
        "### Training (4 points)\n",
        "\n",
        "For this task, you have two options:\n",
        "\n",
        "__Option A:__ fine-tune your own model. You are free to choose any model __except for the original BERT.__ We recommend [DeBERTa-v3](https://huggingface.co/microsoft/deberta-v3-base). Better yet, choose the best model based on public benchmarks (e.g. [GLUE](https://gluebenchmark.com/)).\n",
        "\n",
        "You can write the training code manually or use transformers.Trainer (see [this example](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification)). Please make sure that your model's accuracy is at least __comparable__ with the above example for BERT.\n",
        "\n",
        "\n",
        "__Option B:__ compare at least 3 pre-finetuned models (in addition to the above BERT model). For each model, report (1) its accuracy, (2) its speed, measured in samples per second in your hardware setup and (3) its size in megabytes. Please take care to compare models in equal setting, e.g. same CPU / GPU. Compile your results into a table and write a short (~half-page on top of a table) report, summarizing your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHsPQwHUR3z7"
      },
      "source": [
        "**Task 2 (4 points)**\n",
        "- Choose Option A or Option B (only one will be graded)\n",
        "- Follow all the instructions and restrictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749,
          "referenced_widgets": [
            "2201b86d081b45d1b245a91c7bcb5e42",
            "681e479bd2fa4076bb837b221bdfb563",
            "520acc1cee664fec8d6e3611c6005d8f",
            "077f122ee2514d0f8e9a000c808eb2b8",
            "e5a8e0576e3747439e44920b27c67f23",
            "bf7f17e8640b494e8562547fdd9b1553",
            "393735ef4afa4d21aa4957b98b1791bf",
            "2fb7fc527f3143d7ad86bb8705b9247f",
            "aeb94a689aec4b54be2347aac3f27c9a",
            "979982e6eb6448d9a7b381d147ab0c72",
            "ab3c5d6ca6b345ec83e1675ab8b99858",
            "08fbe0f6a6c94e2cbac5736305e3d8c1",
            "022420360ad146a7a4bbabfd0e10fa26",
            "84d5c92268b84400b3fd9c13c80596a2",
            "b865d20e208743f4a52642bd4012cc41",
            "f7d3de8d6ddd46e388b878c6c83f60d5",
            "23e27e6abc294b7dbaef288cc7cadebc",
            "58db36ff071b4381b0e6c9c8865e3e4e",
            "ee703418e23a4a508c9bdcd87c0eb861",
            "1612264511144cfebdcfddcc37d05d7f",
            "f0424b4a6e764fb0b7c2b66259772ad7",
            "be8316117635460ea2429f22a05d56d1"
          ]
        },
        "id": "T0ZkZTkl_yMU",
        "outputId": "6dc1653f-3dc6-4da3-c891-748fd7581e38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using model: microsoft/deberta-v3-small\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded. Number of parameters: 141,896,450\n",
            "Tokenizing datasets...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2201b86d081b45d1b245a91c7bcb5e42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/363846 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08fbe0f6a6c94e2cbac5736305e3d8c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40430 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 10000\n",
            "Validation samples: 1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on device: cuda\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1250/1250 02:09, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.375500</td>\n",
              "      <td>0.335523</td>\n",
              "      <td>0.862000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed successfully!\n",
            "\n",
            "Evaluating on validation set...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Validation Results:\n",
            "eval_loss: 0.3355\n",
            "eval_accuracy: 0.8620\n",
            "eval_runtime: 1.1581\n",
            "eval_samples_per_second: 863.4580\n",
            "eval_steps_per_second: 54.3980\n",
            "epoch: 1.0000\n",
            "Saving model...\n",
            "\n",
            "Testing inference...\n",
            "'What is machine learning?' vs 'What is ML?' -> NOT DUPLICATE (confidence: 0.976)\n",
            "'How to cook pasta?' vs 'What is quantum physics?' -> NOT DUPLICATE (confidence: 0.999)\n",
            "'Best pizza in New York' vs 'Top pizza places in NYC' -> DUPLICATE (confidence: 0.888)\n",
            "\n",
            "✅ Final accuracy: 0.8620\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Choose DeBERTa-v3-small for faster training (or use base if you prefer)\n",
        "model_name = \"microsoft/deberta-v3-small\"  # Smaller model for faster training\n",
        "print(f\"Using model: {model_name}\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2,\n",
        ")\n",
        "\n",
        "print(f\"Model loaded. Number of parameters: {model.num_parameters():,}\")\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text1\"],\n",
        "        examples[\"text2\"],\n",
        "        padding=False,\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "    )\n",
        "\n",
        "# Tokenize datasets\n",
        "print(\"Tokenizing datasets...\")\n",
        "train_dataset = qqp[\"train\"].map(tokenize_function, batched=True, remove_columns=[\"text1\", \"text2\", \"idx\", \"label_text\"])\n",
        "val_dataset = qqp[\"validation\"].map(tokenize_function, batched=True, remove_columns=[\"text1\", \"text2\", \"idx\", \"label_text\"])\n",
        "\n",
        "# Rename label column to labels\n",
        "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
        "val_dataset = val_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# Use smaller subset for faster training\n",
        "train_dataset = train_dataset.select(range(10000))  # 10k samples\n",
        "val_dataset = val_dataset.select(range(1000))       # 1k samples\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Evaluation function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
        "\n",
        "# Simplified training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./deberta-qqp-finetuned\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=8,  # Smaller batch size\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=200,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",  # Evaluate each epoch instead of steps\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    greater_is_better=True,\n",
        "    remove_unused_columns=True,\n",
        "    dataloader_num_workers=0,  # Set to 0 to avoid multiprocessing issues\n",
        "    fp16=False,  # Disable mixed precision to avoid issues\n",
        "    gradient_checkpointing=False,  # Disable gradient checkpointing\n",
        "    report_to=[],\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    processing_class=tokenizer,  # Use processing_class instead of tokenizer\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on device: {device}\")\n",
        "\n",
        "# Clear any cached gradients\n",
        "if hasattr(model, 'zero_grad'):\n",
        "    model.zero_grad()\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "try:\n",
        "    train_result = trainer.train()\n",
        "    print(\"Training completed successfully!\")\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    print(\"\\nEvaluating on validation set...\")\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    print(f\"\\nFinal Validation Results:\")\n",
        "    for key, value in eval_results.items():\n",
        "        print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "    final_accuracy = eval_results[\"eval_accuracy\"]\n",
        "\n",
        "    # Save the model\n",
        "    print(\"Saving model...\")\n",
        "    trainer.save_model(\"./deberta-qqp-final\")\n",
        "    tokenizer.save_pretrained(\"./deberta-qqp-final\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Training error: {e}\")\n",
        "    print(\"Attempting basic evaluation...\")\n",
        "\n",
        "    # Try manual evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Create a simple dataloader for evaluation\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    eval_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=16,\n",
        "        collate_fn=data_collator\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Manual evaluation\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            correct += (predictions == batch[\"labels\"]).sum().item()\n",
        "            total += batch[\"labels\"].size(0)\n",
        "\n",
        "    final_accuracy = correct / total\n",
        "    print(f\"Manual evaluation accuracy: {final_accuracy:.4f}\")\n",
        "\n",
        "# Test inference\n",
        "print(\"\\nTesting inference...\")\n",
        "model.eval()\n",
        "test_samples = [\n",
        "    (\"What is machine learning?\", \"What is ML?\"),\n",
        "    (\"How to cook pasta?\", \"What is quantum physics?\"),\n",
        "    (\"Best pizza in New York\", \"Top pizza places in NYC\")\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for text1, text2 in test_samples:\n",
        "        inputs = tokenizer(text1, text2, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        model = model.to(device)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "        predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "        confidence = predictions[0][predicted_class].item()\n",
        "\n",
        "        result = \"DUPLICATE\" if predicted_class == 1 else \"NOT DUPLICATE\"\n",
        "        print(f\"'{text1}' vs '{text2}' -> {result} (confidence: {confidence:.3f})\")\n",
        "\n",
        "print(f\"\\n✅ Final accuracy: {final_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQD0IV44LrSs"
      },
      "source": [
        "### Finding Duplicates (1 point)\n",
        "\n",
        "Finally, it is time to use your model to find duplicate questions.\n",
        "Please implement a function that takes a question and finds top-5 potential duplicates in the training set. For now, it is fine if your function is slow, as long as it yields correct results.\n",
        "\n",
        "Showcase how your function works with at least 5 examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM5WXW8hSl7H"
      },
      "source": [
        "**Task 3 (1 point)**\n",
        "- Implement function for finding duplicates\n",
        "- Test it on several examples (at least 5)\n",
        "- Check suggested duplicates and make a conclusion about model correctness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLSjmsKaUyQb",
        "outputId": "9dadd89a-34d1-4164-e303-3078e3cf9e4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing question list...\n",
            "Total unique questions: 9821\n",
            "DUPLICATE DETECTION RESULTS\n",
            "\n",
            " Query 1: 'How do I learn programming?'\n",
            "----------------------------------------\n",
            "Searching for duplicates of: 'How do I learn programming?'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing similarities: 100%|██████████| 9821/9821 [02:39<00:00, 61.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 potential duplicates:\n",
            "1. [0.898] How should you start learning programming?\n",
            "2. [0.864] How can I learn programming from scratch?\n",
            "3. [0.585] How would demonetizing 500 and 1000 rupee notes and introducing new 2000 rupee notes help curb black money and corruption?\n",
            "4. [0.565] Could dark matter fill 'empty' space and be displaced by matter? Could the Milky Way's halo be the state of displacement of the dark matter?\n",
            "5. [0.493] If the Indian government has decided to demonetise 500 and 1000 rupee notes, why are they bringing back new 500 and 2000 Rs notes?\n",
            "\n",
            " Query 2: 'What is the best way to lose weight?'\n",
            "----------------------------------------\n",
            "Searching for duplicates of: 'What is the best way to lose weight?'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing similarities: 100%|██████████| 9821/9821 [02:34<00:00, 63.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 potential duplicates:\n",
            "1. [0.963] What are the best simple ways to loose weight?\n",
            "2. [0.963] What are the best ways to lose weight?\n",
            "3. [0.963] What is the best and quick way to lose weight?\n",
            "4. [0.962] What are the best way of loose the weight?\n",
            "5. [0.962] What is the fastest way to lose weight?\n",
            "\n",
            " Query 3: 'How can I make money online?'\n",
            "----------------------------------------\n",
            "Searching for duplicates of: 'How can I make money online?'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing similarities: 100%|██████████| 9821/9821 [02:35<00:00, 63.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 potential duplicates:\n",
            "1. [0.960] How can I realistically make money online?\n",
            "2. [0.960] How can i make money online easily?\n",
            "3. [0.960] How do I can make extra money online?\n",
            "4. [0.960] How could I make money online?\n",
            "5. [0.960] How can one make money online?\n",
            "\n",
            " Query 4: 'What are good programming languages to learn?'\n",
            "----------------------------------------\n",
            "Searching for duplicates of: 'What are good programming languages to learn?'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing similarities: 100%|██████████| 9821/9821 [02:35<00:00, 63.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 potential duplicates:\n",
            "1. [0.939] What is the best programming language for beginners to learn?\n",
            "2. [0.936] What is the best programming/coding language to learn?\n",
            "3. [0.915] What are the best programming languages for beginners and why?\n",
            "4. [0.913] Which is the best programming language for beginners?\n",
            "5. [0.910] What are the best programming languages to learn today?\n",
            "\n",
            " Query 5: 'How do I improve my English?'\n",
            "----------------------------------------\n",
            "Searching for duplicates of: 'How do I improve my English?'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing similarities: 100%|██████████| 9821/9821 [02:35<00:00, 63.16it/s]\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 potential duplicates:\n",
            "1. [0.962] How can I improve my English Language?\n",
            "2. [0.960] How can I improve my English speaking ability?\n",
            "3. [0.960] How could I improve my English?\n",
            "4. [0.960] How can I continue to improve my English?\n",
            "5. [0.959] How I can improve my English communication?\n",
            "\n",
            " Testing on known duplicate pair:\n",
            "Question 1: 'Why are people so obsessed with having a girlfriend/boyfriend?'\n",
            "Question 2: 'How can a single male have a child?'\n",
            "Actual label: NOT DUPLICATE\n",
            "Model prediction: NOT DUPLICATE (confidence: 0.003)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "def find_duplicates(query_question, model, tokenizer, questions_list, top_k=5):\n",
        "    scores = []\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Searching for duplicates of: '{query_question}'\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for candidate in tqdm(questions_list, desc=\"Computing similarities\"):\n",
        "            if candidate.strip().lower() == query_question.strip().lower():\n",
        "                continue  # Skip identical questions\n",
        "\n",
        "            # Tokenize pair of questions\n",
        "            inputs = tokenizer(\n",
        "                query_question,\n",
        "                candidate,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=128\n",
        "            )\n",
        "\n",
        "            # Move to GPU if available\n",
        "            if torch.cuda.is_available():\n",
        "                inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "                model = model.cuda()\n",
        "\n",
        "            # Get model prediction\n",
        "            outputs = model(**inputs)\n",
        "            probs = F.softmax(outputs.logits, dim=-1)\n",
        "            duplicate_prob = probs[0][1].item()  # Probability of being duplicate\n",
        "\n",
        "            scores.append((candidate, duplicate_prob))\n",
        "\n",
        "    # Sort by duplicate probability\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    return scores[:top_k]\n",
        "\n",
        "# Prepare question list from training data\n",
        "print(\"Preparing question list...\")\n",
        "questions_list = []\n",
        "for example in qqp[\"train\"].select(range(5000)):\n",
        "    questions_list.extend([example[\"text1\"], example[\"text2\"]])\n",
        "\n",
        "# Remove duplicates and empty questions\n",
        "questions_list = list(set([q.strip() for q in questions_list if q.strip()]))\n",
        "print(f\"Total unique questions: {len(questions_list)}\")\n",
        "\n",
        "# Test queries\n",
        "test_queries = [\n",
        "    \"How do I learn programming?\",\n",
        "    \"What is the best way to lose weight?\",\n",
        "    \"How can I make money online?\",\n",
        "    \"What are good programming languages to learn?\",\n",
        "    \"How do I improve my English?\"\n",
        "]\n",
        "\n",
        "print(\"DUPLICATE DETECTION RESULTS\")\n",
        "\n",
        "for i, query in enumerate(test_queries, 1):\n",
        "    print(f\"\\n Query {i}: '{query}'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    try:\n",
        "        duplicates = find_duplicates(query, model, tokenizer, questions_list, top_k=5)\n",
        "\n",
        "        print(\"Top 5 potential duplicates:\")\n",
        "        for rank, (question, score) in enumerate(duplicates, 1):\n",
        "            print(f\"{rank}. [{score:.3f}] {question}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "# Test on actual duplicate pair from validation\n",
        "print(f\"\\n Testing on known duplicate pair:\")\n",
        "val_example = qqp[\"validation\"][3]  # We know this is a duplicate from earlier\n",
        "text1, text2 = val_example[\"text1\"], val_example[\"text2\"]\n",
        "\n",
        "print(f\"Question 1: '{text1}'\")\n",
        "print(f\"Question 2: '{text2}'\")\n",
        "print(f\"Actual label: {'DUPLICATE' if val_example['label'] == 1 else 'NOT DUPLICATE'}\")\n",
        "\n",
        "# Test our model on this pair\n",
        "inputs = tokenizer(text1, text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "if torch.cuda.is_available():\n",
        "    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    probs = F.softmax(outputs.logits, dim=-1)\n",
        "    predicted_prob = probs[0][1].item()\n",
        "    prediction = \"DUPLICATE\" if predicted_prob > 0.5 else \"NOT DUPLICATE\"\n",
        "\n",
        "print(f\"Model prediction: {prediction} (confidence: {predicted_prob:.3f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozyCH_brerGb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
