{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zriTdjauH8iQ"
      },
      "outputs": [],
      "source": [
        "%pip install -q transformers huggingface_hub\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwmTTyjUGqol"
      },
      "source": [
        "### Build-a-transformer\n",
        "\n",
        "In this section, you will implement a transformer language model layer by layer, then use it to generate (hopefully) coherent text.\n",
        "\n",
        "To understand how these layers work, please check out our guide to transformers from [nlp course for you -> transformers](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#transformer_intro).\n",
        "\n",
        "\n",
        "First, we download pre-trained weights for the [GPT2 model by OpenAI](https://openai.com/research/better-language-models) - a prominent model from 2019.\n",
        "\n",
        "\n",
        "\n",
        "Idea & code by: Ilya Beletsky"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOcK0lGTGqol",
        "outputId": "6d8547e2-5162-452b-bcb7-ee6aa0d77867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: ['h.0.attn.c_attn.bias', 'h.0.attn.c_attn.weight', 'h.0.attn.c_proj.bias', 'h.0.attn.c_proj.weight', 'h.0.ln_1.bias', 'h.0.ln_1.weight', 'h.0.ln_2.bias', 'h.0.ln_2.weight', 'h.0.mlp.c_fc.bias', 'h.0.mlp.c_fc.weight', 'h.0.mlp.c_proj.bias', 'h.0.mlp.c_proj.weight', 'h.1.attn.c_attn.bias', 'h.1.attn.c_attn.weight', 'h.1. ...\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "state_dict = torch.load(hf_hub_download(\"gpt2\", filename=\"pytorch_model.bin\"))\n",
        "for key, value in tuple(state_dict.items()):\n",
        "    if key.startswith('h.') and key.endswith('.weight') and value.ndim == 2:\n",
        "        value.transpose_(1, 0)  # <-- for compatibility with modern PyTorch modules\n",
        "    if key.startswith('h.') and key.endswith('.attn.bias') and value.ndim == 4:\n",
        "        state_dict.pop(key)  # <-- triangular binar masks, not needed in this code\n",
        "\n",
        "print('Weights:', repr(sorted(state_dict.keys()))[:320], '...')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr0SUtQnGqom"
      },
      "source": [
        "In the next few cells, we shall implement the model layer by layer to make use of those weights.\n",
        "\n",
        "As you might recall, transformers contain two main layer types: attention and fully-connected layers.\n",
        "\n",
        "The fully connected layers are by far easier to understand, so we shall begin there:\n",
        "\n",
        "Please implement fully-connected layer __without residual or layer normalization__ (we'll add those in a bit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3Rh-6DX9Gqom"
      },
      "outputs": [],
      "source": [
        "class GeLUThatWasUsedInGPT2(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * x ** 3)))\n",
        "\n",
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, dim: int):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(dim, 4  * dim)\n",
        "        self.gelu = GeLUThatWasUsedInGPT2()\n",
        "        self.c_proj = nn.Linear(4 * dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape = [batch_size, seq_length, dim]\n",
        "        out = self.c_fc(x)\n",
        "        out = self.gelu(out)\n",
        "        out = self.c_proj(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSVGKnHBGqom"
      },
      "source": [
        "Now, let's test that it works with GPT-2 weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CoWjZwZkGqom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a15b7bf4-cf50-452f-e479-c1a7e96b3c71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seems legit!\n"
          ]
        }
      ],
      "source": [
        "mlp = FullyConnected(dim=768)\n",
        "mlp.load_state_dict({'c_fc.weight': state_dict['h.0.mlp.c_fc.weight'],\n",
        "                     'c_fc.bias': state_dict['h.0.mlp.c_fc.bias'],\n",
        "                     'c_proj.weight': state_dict['h.0.mlp.c_proj.weight'],\n",
        "                     'c_proj.bias': state_dict['h.0.mlp.c_proj.bias']})\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "x = torch.randn(1, 2, 768)  # [batch_size, sequence_length, dim]\n",
        "checksum = torch.sum(mlp(x) * x)\n",
        "assert abs(checksum.item() - 1282.3315) < 0.1, \"layer outputs do not match reference\"\n",
        "assert torch.allclose(mlp(x[:, (1, 0), :])[:, (1, 0), :], mlp(x)), \"mlp must be permutation-invariant\"\n",
        "print(\"Seems legit!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbfCevRwGqom"
      },
      "source": [
        "Now, let's get to attention layers.\n",
        "\n",
        "Since GPT-2 needs to generate text from left to right, each generated token can only attend to tokens on the left (and itself). This kid of attention is called \"Masked\" self-attention, because it hides tokens to the right.\n",
        "\n",
        "As before, please implement masked self-attention __without layernorm or residual connections.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T6j7M4hLGqon"
      },
      "outputs": [],
      "source": [
        "class MaskedSelfAttention(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.c_attn = nn.Linear(dim, dim * 3)  # query + key + value, combined\n",
        "        self.c_proj = nn.Linear(dim, dim)  # output projection\n",
        "        self.dim, self.num_heads = dim, num_heads\n",
        "        self.head_size = dim // num_heads\n",
        "\n",
        "    def forward(self, x):\n",
        "        q, k, v = self.c_attn(x).split(dim=-1, split_size=self.dim)\n",
        "        assert q.shape == k.shape == v.shape == x.shape, \"q, k and v must have the same shape as x\"\n",
        "\n",
        "\n",
        "        # Note: this is an inefficient implementation that uses a for-loop.\n",
        "        # To get the full grade during homework, please re-implement this code:\n",
        "        # 1) do not use for-loops (or other loops). Compute everything in parallel with vectorized operations\n",
        "        # 2) do not use F.scaled_dot_product_attention - write your own attention code using basic PyTorch ops\n",
        "        head_outputs = []\n",
        "        for head_index in range(self.num_heads):\n",
        "            head_selector = range(self.head_size * head_index, self.head_size * (head_index + 1))\n",
        "\n",
        "            head_queries = q[..., head_selector]\n",
        "            head_keys = k[..., head_selector]\n",
        "            head_values = v[..., head_selector]\n",
        "\n",
        "            single_head_output = F.scaled_dot_product_attention(\n",
        "                head_queries, head_keys, head_values,\n",
        "                is_causal=True)\n",
        "            # docs: https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
        "            head_outputs.append(single_head_output)\n",
        "\n",
        "        combined_head_outputs = torch.cat(head_outputs, dim=-1)\n",
        "        return self.c_proj(combined_head_outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umZpcpIkJva7"
      },
      "source": [
        "Test that it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg5Oj_PPM6hj",
        "outputId": "879c19d0-86a5-4000-f890-2fc78a57d8a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It works!\n"
          ]
        }
      ],
      "source": [
        "attn = MaskedSelfAttention(dim=768, num_heads=12)\n",
        "attn.load_state_dict({'c_attn.weight': state_dict['h.0.attn.c_attn.weight'],\n",
        "                      'c_attn.bias': state_dict['h.0.attn.c_attn.bias'],\n",
        "                      'c_proj.weight': state_dict['h.0.attn.c_proj.weight'],\n",
        "                      'c_proj.bias': state_dict['h.0.attn.c_proj.bias']})\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "x = torch.randn(1, 10, 768)  # [batch_size, sequence_length, dim]\n",
        "checksum = torch.sum(attn(x) * x)\n",
        "assert abs(checksum.item() - 2703.6772) < 0.1, \"layer outputs do not match reference\"\n",
        "assert not torch.allclose(attn(x[:, (1, 0), :])[:, (1, 0), :], attn(x[:, (0, 1), :])), \"masked attention must *not* be permutation-invariant\"\n",
        "print(\"It works!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn6tgTHzOK4l"
      },
      "source": [
        "We can now combine attention and MLP to build the full transformer layer:\n",
        "\n",
        "![img](https://i.imgur.com/1sq2vHO.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "p3AH7YQvRpvU"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(dim)\n",
        "        self.attn = MaskedSelfAttention(dim, num_heads)\n",
        "        self.ln_2 = nn.LayerNorm(dim)\n",
        "        self.mlp = FullyConnected(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.ln_1(x)\n",
        "        attn = self.attn(out)\n",
        "        out = x + attn\n",
        "        out1 = self.ln_2(out)\n",
        "        out1 = self.mlp(out1)\n",
        "        return out + out1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qzo_QeFVSNZa",
        "outputId": "1b66f800-4edd-448a-ed35-23222208af55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good job!\n"
          ]
        }
      ],
      "source": [
        "layer = TransformerLayer(dim=768, num_heads=12)\n",
        "layer.load_state_dict({k[5:]: v for k, v in state_dict.items() if k.startswith('h.10.')})\n",
        "assert abs(torch.sum(layer(x) * x).item() - 9874.7383) < 0.1\n",
        "print(\"Good job!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Mbqw9iuaSrYy"
      },
      "outputs": [],
      "source": [
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size: int, dim: int, num_heads: int, num_layers: int, max_position_embeddings: int = 1024):\n",
        "        super().__init__()\n",
        "        self.wte = nn.Embedding(vocab_size, dim)  # token embeddings\n",
        "        self.wpe = nn.Embedding(max_position_embeddings, dim)  # position embeddings\n",
        "        self.ln_f = nn.LayerNorm(dim)   # final layer norm - goes after all transformer layers, but before logits\n",
        "\n",
        "        self.h = nn.Sequential(*(TransformerLayer(dim, num_heads) for layer in range(num_layers)))\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids.shape: [batch_size, sequence_length], int64 token ids\n",
        "        position_ids = torch.arange(input_ids.shape[1], device=input_ids.device).unsqueeze(0)\n",
        "\n",
        "        token_embeddings = self.wte(input_ids)\n",
        "        position_embeddings = self.wpe(position_ids)\n",
        "        full_embeddings = token_embeddings + position_embeddings\n",
        "\n",
        "        transformer_output = self.h(full_embeddings)\n",
        "        transformer_output_ln = self.ln_f(transformer_output)\n",
        "\n",
        "        # final layer: we predict logits by re-using token embeddings as linear weights\n",
        "        output_logits = transformer_output_ln @ self.wte.weight.T\n",
        "        return output_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "p0m8jt66aDIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf2faee-e968-46c1-ec8f-49979018f3a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:  look\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
        "model = GPT2(vocab_size=50257, dim=768, num_heads=12, num_layers=12)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "input_ids = tokenizer(\"A quick\", return_tensors='pt')['input_ids']\n",
        "\n",
        "predicted_logits = model(input_ids)\n",
        "most_likely_token_id = predicted_logits[:, -1].argmax().item()\n",
        "\n",
        "print(\"Prediction:\", tokenizer.decode(most_likely_token_id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8ql3Lo7dXZ2",
        "outputId": "44a1762a-d8a2-41a5-8ca0-c63c330c3f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Fermi paradox  is perhaps the most widely recognized example of the most recent fast-growing age of science.\n",
            " In 2005\n",
            ", a strong new wave of paleoanthropologist Michael Mann published the following graph showing how fast the paleoanthropological\n",
            " record has evolved.  In that year, he calculated that the rate of change in age and stature was about 30% more rapid than\n",
            " that of the recent past.  The graph shows that much of the change in age, along with changes in stature, was due to changes\n",
            " in the timing of ancient changes.  As well, it shows that modern humans are already about twice as fast as they were in the\n",
            " past.\n",
            "And what about the transition to higher productivity?  We are already within the previous 20 to 25 years of modern\n",
            " human growth, and by 30, 80% of the changes in the lifespan of the human population will have occurred in the last 2,000\n",
            " years.\n",
            "Moving along, there are still a few ways in which the rapid decline of the human lifespan may be analogous to the\n",
            " rapid change of the whole human population.  One possibility is that the changes in human populations occurred in a coordinated\n",
            " fashion, although the rate of change proves to be much more stable, perhaps more likely, than previously thought.  The other\n",
            " possibility is that the rapid change in human population occurred in some natural, non-anthropogenic, way.\n",
            "In any case, even\n",
            " though modern humans have already reached the point where they are well above the human population, there are still two very\n",
            " important things to keep in mind: 1) that the nuclear cliff is a very interesting, highly important discovery, and 2) that\n",
            " it is quite possibly a place where the story of the human race will go on for thousands of years.\n",
            "6. The Myth of the Golden\n",
            " Age of Science\n",
            "That(s) Science can be told is not an exaggeration.  It can be made out clearly and convincingly, and it is\n",
            " true that there is a huge range of plausibility in the claims that many of these claims are grounded in reality.  And, of\n",
            " course, it is true that many of these claims are actually based upon ancient fossils, but that is not a single one of them\n",
            ".  It is almost always of interest to learn more about the times and places around us, but it would be a nice thing to remember\n",
            ", and it would be a nice thing to research, to learn more about the living organisms and the habits of their life, and to\n",
            " learn more"
          ]
        }
      ],
      "source": [
        "text = \"The Fermi paradox \"\n",
        "tokens = tokenizer.encode(text)\n",
        "print(end=tokenizer.decode(tokens))\n",
        "line_length = len(tokenizer.decode(tokens))\n",
        "\n",
        "for i in range(500):\n",
        "    # Predict logits with your model\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.as_tensor([tokens])) * 1.3\n",
        "\n",
        "    # Sample with probabilities\n",
        "    p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n",
        "    next_token_index = np.random.choice(len(p_next), p=p_next)\n",
        "\n",
        "    tokens.append(int(next_token_index))\n",
        "    print(end=tokenizer.decode(tokens[-1]))\n",
        "    line_length += len(tokenizer.decode(tokens[-1]))\n",
        "    if line_length > 120:\n",
        "      line_length = 0\n",
        "      print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3NJ0ocgGqop"
      },
      "source": [
        "__Reminder:__ after class, please go to `MaskedSelfAttention.forward` above and finish the job!\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "### Here's how you can do the same with transformers library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NTOHu124Gqop",
        "outputId": "0a7bb238-8015-481c-d4bb-ef2d6401245f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:  The Fermi paradox  was the original paradox of the Fermi Paradox but the Fermi paradox was also the first one to emerge from the vacuum and was known as the Fermi Paradox , and a new paradox became known as the Fermi Paradox.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "print('Generated text:', tokenizer.decode(\n",
        "    model.generate(\n",
        "        **tokenizer(\"The Fermi paradox \", return_tensors='pt'),\n",
        "        do_sample=True, max_new_tokens=50\n",
        "    ).flatten().numpy(),\n",
        "    skip_special_tokens=True\n",
        "))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another approach to gen text from model output logits as in example above."
      ],
      "metadata": {
        "id": "7QINlyzKNoT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The Fermi paradox \"\n",
        "tokens = tokenizer.encode(text)\n",
        "print(end=tokenizer.decode(tokens))\n",
        "line_length = len(tokenizer.decode(tokens))\n",
        "\n",
        "for i in range(500):\n",
        "    # Predict logits with your model\n",
        "    with torch.no_grad():\n",
        "        logits = model(torch.as_tensor([tokens])).logits * 1.3\n",
        "\n",
        "    # Sample with probabilities\n",
        "    p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n",
        "    next_token_index = np.random.choice(len(p_next), p=p_next)\n",
        "\n",
        "    tokens.append(int(next_token_index))\n",
        "    print(end=tokenizer.decode(tokens[-1]))\n",
        "    line_length += len(tokenizer.decode(tokens[-1]))\n",
        "    if line_length > 120:\n",
        "      line_length = 0\n",
        "      print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUDJjV7tFl-j",
        "outputId": "459a475a-c61a-4143-bf93-dcd70c5c5154"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Fermi paradox  is the most concrete example of this effect. This paradox is preserved by the nonlinearity of space. In\n",
            " other words, the time-space of an object (and sometimes the interval between objects) is always determined by the time of\n",
            " its initial step, and if it is only a few moments, then a finite amount of time will have elapsed. It can thus be observed\n",
            " that, in particular, when a given moment of the time-space of a point is discrete, the interval between a point's points\n",
            " must be finite, and there needs to be enough time between those points to achieve sufficiently large amounts of time. The\n",
            " existence of such a paradox implies that, even in conditions of absolute time equilibrium, we can be held to be infinitely\n",
            " well-informed about the time-space of our little quadrants of matter. We can simply assume that the space of our little qu\n",
            "bits is finite. But this assumption is contradicted by the fact that, when a point has an interval of restricted time between\n",
            " points, the interval between that interval and the time-space of a point is finite, and the interval between that interval\n",
            " and the time-space of a point is finite. So we must assume the time-space of a point to be finite. In other words, the interval\n",
            " between a point's points and its interval of time is finite. This is a first step from certain natural order to certain natural\n",
            " order. It is only in this context that the paradox is exposed. Not only does it provide an illustration of an important paradox\n",
            ", but it also makes it simple to see that, for the purpose of presenting the paradox in a more general way, we should focus\n",
            " on an aspect of our natural order that is deeply and completely contradictory to that of any theory as simple as the zen\n",
            "ith and beyond.\n",
            "The Here and Now Principle\n",
            "In this section, I will attempt to show how the present paper is a part of Nature\n",
            "'s approach to the solution to the Fermi paradox. The paper does not attempt to persuade us that, through natural selection\n",
            ", all of our tiny qubits can be made infinite  by natural selection. It simply asserts that the number of extra qubits in\n",
            " the universe (in spite of all their inherent computational complexity) does not imply that they will be infinite. For this\n",
            " reason, it is important to note that at this point, we can be sure that the addition of a new qubit to our qubits does not\n",
            " imply that they will have anything to do"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wp5-6pKxGHdw"
      },
      "execution_count": 13,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}